{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document distances and clustering\n",
    "\n",
    "This notebook was posted by Simon Lindgren // [@simonlindgren](http://www.twitter.com/simonlindgren) // [simonlindgren.com](http://simonlindgren.com)\n",
    "\n",
    "The following code is about how to calculate and visualise measures of distance and similarity between documents in a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix). I have created [another notebook](https://github.com/simonlindgren/TXT-to-DTM/blob/master/TXT%20to%20DTM.ipynb) about how to create such a matrix from a set of text documents.\n",
    "\n",
    "When creating this notebook, I drew on the tutorial package [TAToM](https://de.dariah.eu/tatom/), which was written by [Allen Riddell](http://www.twitter.com/ariddell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTM recap\n",
    "The first block of code below is a quick version of the procedure that I described in the [previous notebook](https://github.com/simonlindgren/TXT-to-DTM/blob/master/TXT%20to%20DTM.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "filenames = []\n",
    "for f in glob.glob('/Users/simon/AnacondaProjects/data/*txt'):\n",
    "    filenames.append(f)\n",
    "\n",
    "# Define custom stop words\n",
    "stoplist = [\"hej\", \"och\"]\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename', stop_words='english', min_df=1)\n",
    "dtm = vectorizer.fit_transform(filenames)  # a sparse matrix\n",
    "vocab = vectorizer.get_feature_names() # a vocaculary list\n",
    "\n",
    "len(vocab) # check length of the vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean distances\n",
    "With the documents converted into [vectors](http://en.wikipedia.org/wiki/Euclidean_vector#History), we can use mathematical notions of similarity and distance between these documents.\n",
    "\n",
    "Let's calculate [Euclidean distances](https://en.wikipedia.org/wiki/Euclidean_distance) with the helap of the `sklearn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "dist = euclidean_distances(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have information about distances between documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document 1 in our dtm\n",
    "filenames[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document 3 in our dtm\n",
    "filenames[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the distance between these two documents\n",
    "dist[1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarities\n",
    "\n",
    "We can also calculate [cosine similarities](https://en.wikipedia.org/wiki/Cosine_similarity), which helps us get a measure of distance that takes the length of the documents into consideration. This is of course a good idea.\n",
    "\n",
    "The code for this (below) is very similar to that of Euclidean distance (above). But as cosine similarity is a measure of (yes) _similarity_, it must be flipped in order to be used as a measure of _distance_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(dtm) # calculate and flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist[1, 3] # try it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the distances in 2D\n",
    "It is possible to get a visual representation of the pairwise distances between documents. This is done by assigning a point in a plane to each text, and then making sure that the distance between these points is proportional to the distances that we calculated. This strategy is called [multidimensional scaling](https://en.wikipedia.org/wiki/Multidimensional_scaling) (MDS). In the code below, we set `n_components` to 2, because we are plotting in a two-dimensional plane. Specifying a [`random_state`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.RandomState.html) makes the plot reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "# Shorten the document filenames for prettier display\n",
    "names = [os.path.basename(fn).replace('.txt', '') for fn in filenames]\n",
    "\n",
    "# Define the plot\n",
    "for x, y, name in zip(xs, ys, names):\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x, y, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the distances in 3D\n",
    "Below follows similar code as above, but fitted for 3D visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mds = MDS(n_components=3, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(pos[:, 0], pos[:, 1], pos[:, 2])\n",
    "\n",
    "for x, y, z, s in zip(pos[:, 0], pos[:, 1], pos[:, 2], names):\n",
    "    ax.text(x, y, z, s)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster visualisation\n",
    "It is often useful in exploratory text analysis to cluster texts into groups of similar texts. We use the distances that we calculated earlier as a starting point for bringing documents with smaller distances between them into the same cluster. There are many clustering methods, but we will use [Ward's method](https://en.wikipedia.org/wiki/Ward%27s_method). This is a strategy for _hierarchical_ clustering, meaning that it merges close clusters and orders them in a clustering tree, or [dendrogram](https://en.wikipedia.org/wiki/Dendrogram).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "linkage_matrix = ward(dist)\n",
    "dendrogram(linkage_matrix, orientation=\"right\", labels=names)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
